dropout技术是减少过拟合的一种手段，它将我们原有的全连接的网络，随机地删除一些神经元(除了输入层和输出层)，相当于相同数据在不同的网络中训练
平均几种网络的结果，dropout不同设置的神经元和我们训练几种不同的神经网络很像。因此，dropout处理很像是平均一个大量不同网络的平均结果。
不同的网络在不同的情况下过拟合。因此，很大程度上。dropout将会减少这种过拟合。

一个相关的早期使用这种技术的论文（（**ImageNet Classification with Deep Convolutional Neural Networks, 
by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton (2012).））
中启发性的dropout解释是:
这种技术减少了神经元之间复杂的共适性。因为一个神经元不能依赖其他特定的神经元。因此，不得不去学习随机子集神经元间的鲁棒性的有用连接。
换句话说。想象我们的神经元作为要给预测的模型，dropout是一种方式可以确保我们的模型在丢失一个个体线索的情况下保持健壮的模型。
在这种情况下，可以说他的作用和L1和L2范式正则化是相同的。都是来减少权重连接，然后增加网络模型在缺失个体连接信息情况下的鲁棒性。
